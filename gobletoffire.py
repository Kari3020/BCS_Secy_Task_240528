# -*- coding: utf-8 -*-
"""GobletofFire.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12kMylhHlvdnuMrgKe1y5cDWEBqwXl3Hm

# Importing some libraries and hyperparameters
"""

import pygame
import numpy as np
import random
from collections import deque
import pickle

# Hyperparameters
ALPHA = 0.1  # Learning rate
GAMMA = 0.9  # Discount factor
EPSILON_START = 1.0
EPSILON_MIN = 0.01
EPSILON_DECAY = 0.995
EPISODES = 1000

"""# Maze Environment class



"""

class MazeEnvironment:
    def __init__(self, maze_file):
        self.grid = self.load_maze(maze_file)
        self.cup_pos = None
        self.reset()

    def load_maze(self, filename):
        with open(filename, 'r') as f:
            return [list(line.strip()) for line in f]

    def reset(self):
        self.cup_pos = self._find_cup_position()  # FIXED: Set cup first
        self.harry_pos = self._random_valid_position()
        self.death_eater_pos = self._random_valid_position()
        return self._get_state()

    def _random_valid_position(self):
        while True:
            x = random.randint(0, len(self.grid[0])-1)
            y = random.randint(0, len(self.grid)-1)
            if self.grid[y][x] != '#' and (x, y) != self.cup_pos:
                return (x, y)

    def _find_cup_position(self):
        for y, row in enumerate(self.grid):
            for x, cell in enumerate(row):
                if cell == 'C':
                    return (x, y)
        return self._random_valid_position()

    def _get_state(self):
        return (*self.harry_pos, *self.death_eater_pos)

    def step(self, action):
        # Move Harry
        new_harry = self._move(self.harry_pos, action)
        if self._is_valid(new_harry):
            self.harry_pos = new_harry

        if self.harry_pos == self.cup_pos:
            return self._get_state(), 100, True

        # Move Death Eater using BFS
        path = self._bfs(self.death_eater_pos, self.harry_pos)
        if path and len(path) > 1:
            self.death_eater_pos = path[1]

        if self.harry_pos == self.death_eater_pos:
            return self._get_state(), -100, True

        return self._get_state(), -1, False

    def _move(self, pos, action):
        x, y = pos
        actions = {
            0: (0, -1),  # Up
            1: (0, 1),   # Down
            2: (-1, 0),  # Left
            3: (1, 0)    # Right
        }
        dx, dy = actions[action]
        return (x + dx, y + dy)

    def _is_valid(self, pos):
        x, y = pos
        return 0 <= x < len(self.grid[0]) and 0 <= y < len(self.grid) and self.grid[y][x] != '#'

    def _bfs(self, start, target):
        queue = deque()
        queue.append([start])
        visited = set()
        visited.add(start)

        while queue:
            path = queue.popleft()
            x, y = path[-1]

            if (x, y) == target:
                return path

            for dx, dy in [(0,1), (0,-1), (1,0), (-1,0)]:
                new_x, new_y = x + dx, y + dy
                new_pos = (new_x, new_y)
                if self._is_valid(new_pos) and new_pos not in visited:
                    visited.add(new_pos)
                    queue.append(path + [new_pos])
        return None

"""# QLearningAgent Class"""

class QLearningAgent:
    def __init__(self, env):
        self.env = env
        self.q_table = {}
        self.epsilon = EPSILON_START

    def get_action(self, state):
        if random.random() < self.epsilon:
            return random.randint(0, 3)

        q_values = [self.q_table.get((state, a), 0) for a in range(4)]
        return np.argmax(q_values)

    def update_q_table(self, state, action, reward, next_state):
        old_value = self.q_table.get((state, action), 0)
        next_max = max([self.q_table.get((next_state, a), 0) for a in range(4)])
        new_value = (1 - ALPHA) * old_value + ALPHA * (reward + GAMMA * next_max)
        self.q_table[(state, action)] = new_value

        self.epsilon = max(EPSILON_MIN, self.epsilon * EPSILON_DECAY)

"""# Pygame Visualization"""

class MazeVisualizer:
    def __init__(self, env, cell_size=40):
        self.env = env
        self.cell_size = cell_size
        pygame.init()
        self.screen = pygame.display.set_mode(
            (len(env.grid[0])*cell_size, len(env.grid)*cell_size))

    def draw(self):
        self.screen.fill((30, 30, 30))

        for y, row in enumerate(self.env.grid):
            for x, cell in enumerate(row):
                rect = pygame.Rect(x*self.cell_size, y*self.cell_size,
                                 self.cell_size, self.cell_size)
                if cell == '#':
                    pygame.draw.rect(self.screen, (50, 50, 50), rect)

        pygame.draw.circle(self.screen, (0, 255, 0),
                          (int((self.env.harry_pos[0]+0.5)*self.cell_size),
                           int((self.env.harry_pos[1]+0.5)*self.cell_size)),
                          int(self.cell_size/3))

        pygame.draw.circle(self.screen, (255, 0, 0),
                          (int((self.env.death_eater_pos[0]+0.5)*self.cell_size),
                           int((self.env.death_eater_pos[1]+0.5)*self.cell_size)),
                          int(self.cell_size/3))

        pygame.draw.rect(self.screen, (255, 255, 0),
                        pygame.Rect((self.env.cup_pos[0]+0.25)*self.cell_size,
                                   (self.env.cup_pos[1]+0.25)*self.cell_size,
                                   self.cell_size/2, self.cell_size/2))

        pygame.display.flip()

"""# Training Loop"""

def train():
    env = MazeEnvironment("maze.txt")
    agent = QLearningAgent(env)
    visualizer = MazeVisualizer(env)

    success_history = []
    reward_history = []
    consecutive_wins = 0

    for episode in range(EPISODES):
        state = env.reset()
        done = False
        total_reward = 0

        while not done:
            action = agent.get_action(state)
            next_state, reward, done = env.step(action)
            agent.update_q_table(state, action, reward, next_state)
            state = next_state
            total_reward += reward

            if episode % 100 == 0:
                visualizer.draw()
                pygame.time.wait(50)

        reward_history.append(total_reward)

        if reward == 100:
            consecutive_wins += 1
            if consecutive_wins >= 10:
                print(f"Trained successfully at episode {episode}")
                break
        else:
            consecutive_wins = 0

        success_history.append(1 if reward == 100 else 0)

    # Save Q-table using pickle
    with open("q_table.pkl", "wb") as f:
        pickle.dump(agent.q_table, f)

        import matplotlib.pyplot as plt
        plt.plot(reward_history)
        plt.title("Total Reward per Episode")
        plt.xlabel("Episode")
        plt.ylabel("Reward")
        plt.grid()
        plt.show()
        plt.figure()

        plt.plot(np.convolve(success_history, np.ones(100)/100, mode='valid'))
        plt.title("Success Rate (Moving Average over 100 episodes)")
        plt.xlabel("Episode")
        plt.ylabel("Success Rate")
        plt.grid()
        plt.show()

    pygame.quit()

if __name__ == "__main__":
    train()